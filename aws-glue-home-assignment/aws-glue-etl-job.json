{
	"jobConfig": {
		"name": "aws-glue-etl-job",
		"description": "",
		"role": "arn:aws:iam::249751718460:role/service-role/AWSGlueServiceRole-s3crawler",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "aws-glue-etl-job.py",
		"scriptLocation": "s3://aws-glue-home-assignment-serverless/etl-glue-script/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--config",
				"value": "{\"s3bucket\": \"aws-glue-home-assignment-serverless\", \"s3_key_athena\": \"athena-dataset\", \"s3_key_src\": \"data/input/stock_prices.csv\", \"s3_key_dst\": {\"report1\": \"data/report1\", \"report2\": \"data/report2\", \"report3\": \"data/report3\", \"report4\": \"data/report4\"}, \"athena_tables\": {\"report1\": {\"table_name\": \"average_return\", \"table_column\": [\"date\", \"average_return\"]}, \"report2\": {\"table_name\": \"frequency\", \"table_column\": [\"ticker\", \"frequency\"]}, \"report3\": {\"table_name\": \"stddev\", \"table_column\": [\"ticker\", \"standard_deviation\"]}, \"report4\": {\"table_name\": \"top3_30_day_return_dates\", \"table_column\": [\"ticker\", \"date\"]}}}",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2023-12-19T22:47:26.297Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-249751718460-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-249751718460-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"script": "\nimport sys\nimport math\nimport json\nimport datetime\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n#import logging\n#logging.basicConfig(level=logging.INFO)\n\ndef load_data(spark, config: dict) -> F.DataFrame:\n    # Load the CSV file into a DataFrame\n    stock_prices_df = spark.read.csv(\n        f's3://{config[\"s3bucket\"]}/{config[\"s3_key_src\"]}',\n        header=True,\n        inferSchema=True\n        )\n    # Convert the date column to date type\n    sp_df = (\n        stock_prices_df\n        .withColumn(\"date\", F.to_date(F.col(\"date\"), \"M/d/yyyy\"))\n        .select(\"date\", \"ticker\", \"volume\", \"close\")\n        )\n    sp_df.printSchema()\n    #sp_df.show()\n    return sp_df\n\n\ndef report1_get_average_return(df: F.DataFrame):\n    # Calculate the daily return for each stock and date\n    window_spec = (\n        Window\n        .partitionBy(\"ticker\")\n        .orderBy(\"date\")\n        )\n    df_daily_returns = (\n        df.withColumn(\"lag_close\", F.lag(\"close\").over(window_spec))\n        .withColumn(\n            \"daily_return\",\n            100 * (F.col(\"close\") - F.col(\"lag_close\")) / F.col(\"lag_close\")\n            )\n        .drop(\"lag_close\")\n        )\n    # Calculate the average daily return for each date\n    df_avg_daily_return = (\n        df_daily_returns\n        .groupBy(\"date\")\n        .agg(F.avg(\"daily_return\").alias(\"average_return\"))\n        .orderBy(\"date\")\n        )\n    df_avg_daily_return = df_avg_daily_return.withColumn(\n        \"average_return\",\n        F.when(F.col(\"average_return\").isNull(), 0)\n        .otherwise(F.col(\"average_return\"))\n        )\n    print(\"# 1 - average daily return of all stocks for every date\")\n    # Display the result\n    df_avg_daily_return.show(truncate=False)\n    return df_avg_daily_return\n\n\ndef report2_get_frequency(df: F.DataFrame):\n    # Calculate closing price * volume\n    df = (\n        df\n        .withColumn(\"close_volume\", F.col(\"close\") * F.col(\"volume\"))\n        .drop(\"close\")\n        .drop(\"volume\")\n        )\n    # Identify the highest closing price * volume for each day\n    window_spec = Window.partitionBy(\"date\").orderBy(F.desc(\"close_volume\"))\n    df = df.withColumn(\"rank\", F.rank().over(window_spec))\n    df = df.filter(\"rank == 1\").drop(\"rank\")\n\n    # Count the frequency for each stock\n    frequency_df = df.groupBy(\"ticker\").agg(F.count(\"date\").alias(\"frequency\"))\n    # Calculate the total number of days in the chosen timeframe\n    total_days = df.select(\"date\").distinct().count()\n    # Calculate the average frequency\n    frequency_df = (\n        frequency_df\n        .withColumn(\"frequency\", F.col(\"frequency\") / total_days)\n        )\n    # Identify the stock with the highest average frequency\n    result_df = frequency_df.orderBy(F.desc(\"frequency\")).limit(1)\n    print('# 2 - most frequency over time')\n    # Print the results\n    result_df.select(\"ticker\", \"frequency\").show(truncate=False)\n    return result_df\n\n\ndef report3_get_stddev(df: F.DataFrame):\n    # Calculate the daily return for each stock\n    window_spec = Window.partitionBy(\"ticker\").orderBy(\"date\")\n    df_daily_returns = (\n        df.withColumn(\"lag_close\", F.lag(\"close\").over(window_spec))\n        .withColumn(\n            \"daily_return\",\n            (F.col(\"close\") - F.col(\"lag_close\")) / F.col(\"lag_close\") * 100\n            )\n        .drop(\"lag_close\")\n    )\n    df_daily_returns = df_daily_returns.withColumn(\n        \"daily_return\",\n        F.when(F.col(\"daily_return\").isNull(), 0)\n        .otherwise(F.col(\"daily_return\"))\n        )\n    # Calculate the standard deviation of daily returns for each stock\n    df_std_daily_returns = (\n        df_daily_returns\n        .groupBy(\"ticker\")\n        .agg(F.stddev(\"daily_return\").alias(\"std_daily_return\"))\n    )\n    # Annualize the standard deviation of daily returns\n    trading_days_per_year = 252\n    df_annualized_std = (\n        df_std_daily_returns\n        .withColumn(\n            \"standard_deviation\",\n            F.col(\"std_daily_return\") * math.sqrt(trading_days_per_year)\n            )\n        .orderBy(F.desc(\"standard_deviation\"))\n        .select(\"ticker\", \"standard_deviation\")\n        .limit(1)\n    )\n    print(\"# 3 - most volatile stock:\")\n    # Display the result\n    df_annualized_std.show(truncate=False)\n    return df_annualized_std\n\n\ndef report4_get_30_days_return(df: F.DataFrame):\n    # Calculate the 30-day percentage change for each closing price\n    window_spec = Window.partitionBy(\"ticker\").orderBy(\"date\")\n    df_30day_returns = (\n        df.withColumn(\"lag_close\", F.lag(\"close\", 30).over(window_spec))\n        .withColumn(\"lag_date\", F.lag(\"date\", 30).over(window_spec))\n        .withColumn(\n            \"daily_return\",\n            ((F.col(\"close\") - F.col(\"lag_close\")) / F.col(\"lag_close\")) * 100\n            )\n        .drop(\"lag_close\")\n    )\n    \n    # Rank daily returns by percentage increase for each stock\n    window_spec_top3 = Window.partitionBy(\"ticker\").orderBy(\n        F.desc(\"daily_return\")\n        )\n    df_ranked_returns = (\n        df_30day_returns\n        .withColumn(\"rank\", F.rank().over(window_spec_top3))\n        .filter(F.col(\"rank\") <= 3)\n        .orderBy(\"ticker\", F.desc(\"rank\"))\n        .select(\"ticker\", \"date\")\n    )\n    print(\"# 4 - top three 30-day return dates, per ticker:\")\n    # Display the result\n    df_ranked_returns.show(truncate=False)\n    return df_ranked_returns\n\n\ndef main():\n    # Script generated for node aws-glue-glue-home-assignment-serverless\n    args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"config\"])\n    config = json.loads(args[\"config\"])\n    sc = SparkContext()\n    glue_context = GlueContext(sc)\n    spark = glue_context.spark_session\n    job = Job(glue_context)\n    job.init(args['JOB_NAME'], args)\n    spark = (\n        SparkSession\n        .builder.appName(\"StockPrices\")\n        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n        .getOrCreate()\n        )\n    #result = run_tasks(spark, config)\n    timestamp = datetime.datetime.today().strftime('%Y%m%d')\n    sp_df = load_data(spark, config)\n    average_return = report1_get_average_return(sp_df)\n    dst_report1 = f's3://{config[\"s3bucket\"]}' \\\n                  f'/{config[\"s3_key_dst\"][\"report1\"]}' \\\n                  f'/{timestamp}.csv'\n    average_return.write.csv(\n        dst_report1,\n        header=True,\n        mode=\"overwrite\"\n        )\n    frequency= report2_get_frequency(sp_df)\n    dst_report2 = f's3://{config[\"s3bucket\"]}' \\\n                  f'/{config[\"s3_key_dst\"][\"report2\"]}' \\\n                  f'/{timestamp}.csv'\n    frequency.write.csv(\n        dst_report2,\n        header=True,\n        mode=\"overwrite\"\n        )\n    stddev = report3_get_stddev(sp_df)\n    dst_report3 = f's3://{config[\"s3bucket\"]}' \\\n                  f'/{config[\"s3_key_dst\"][\"report3\"]}' \\\n                  f'/{timestamp}.csv'\n    stddev.write.csv(\n        dst_report3,\n        header=True,\n        mode=\"overwrite\")\n    report4_get_30_days = report4_get_30_days_return(sp_df)\n    dst_report4 = f's3://{config[\"s3bucket\"]}' \\\n                  f'/{config[\"s3_key_dst\"][\"report4\"]}' \\\n                  f'/{timestamp}.csv'\n    report4_get_30_days.write.csv(\n        dst_report4,\n        header=True,\n        mode=\"overwrite\")\n    job.commit()\n    spark.stop()\n\nif __name__ == \"__main__\":\n    main()\n"
}